import gradio as gr
import requests
import json
from typing import Optional


class VLLMClient:
    def __init__(self, server_url="http://localhost:8000",
                 model_name="qwen-lora-awq", api_key: Optional[str] = None):
        self.server_url = server_url
        self.model_name = model_name
        self.api_key = api_key
        self.headers = {"Content-Type": "application/json"}
        if api_key:
            self.headers["Authorization"] = f"Bearer {api_key}"

    def stream_generation(self, messages, temperature=0.7, max_tokens=800,
                          top_p=0.9):
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        url = f"{self.server_url}/v1/chat/completions"

        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": True
        }

        try:
            response = requests.post(url, json=payload, headers=self.headers,
                                     stream=True)
            response.raise_for_status()

            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data != '[DONE]':
                            try:
                                chunk = json.loads(data)
                                if 'choices' in chunk and chunk['choices']:
                                    delta = chunk['choices'][0].get('delta',
                                                                    {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                continue
        except Exception as e:
            yield f"é”™è¯¯: {str(e)}"


# åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹ - å¾®è°ƒå‰å’Œå¾®è°ƒå
client_before = VLLMClient(server_url="http://192.168.182.79:8099",
                           model_name="qwen32-ds-v3-icf-int4",
                           api_key="gpustack_7a1039ec63b76881_ec45a4f760292ce227970bc4306db740")  # å¾®è°ƒå‰æ¨¡å‹
client_after = VLLMClient(model_name="qwen-lora-awq")  # å¾®è°ƒåæ¨¡å‹

# æ¨¡å‹æ˜¾ç¤ºåç§°
MODEL_NAMES = {
    "before": {
        "display_name": "ğŸ¯ å¾®è°ƒå‰æ¨¡å‹",
        "server_info": f"æœåŠ¡åœ°å€: {client_before.server_url}",
        "model_info": f"æ¨¡å‹åç§°: {client_before.model_name}"
    },
    "after": {
        "display_name": "ğŸš€ å¾®è°ƒåæ¨¡å‹",
        "server_info": f"æœåŠ¡åœ°å€: {client_after.server_url}",
        "model_info": f"æ¨¡å‹åç§°: {client_after.model_name}"
    }
}


def convert_to_gradio_format(history):
    """å°†å†å²è®°å½•è½¬æ¢ä¸º Gradio æ ¼å¼"""
    gradio_history = []
    for user_msg, assistant_msg in history:
        gradio_history.append({"role": "user", "content": user_msg})
        gradio_history.append({"role": "assistant", "content": assistant_msg})
    return gradio_history


def convert_from_gradio_format(gradio_history):
    """å°† Gradio æ ¼å¼è½¬æ¢å›å†…éƒ¨åˆ—è¡¨æ ¼å¼"""
    history = []
    for i in range(0, len(gradio_history), 2):
        if i + 1 < len(gradio_history):
            user_msg = gradio_history[i]["content"]
            assistant_msg = gradio_history[i + 1]["content"]
            history.append([user_msg, assistant_msg])
    return history


def stream_chat_compare(message, history_before, history_after, temperature,
                        max_tokens):
    """æµå¼èŠå¤©å‡½æ•° - å¯¹æ¯”æ¨¡å¼"""
    # å°† Gradio æ ¼å¼è½¬æ¢å›å†…éƒ¨æ ¼å¼
    history_before_internal = convert_from_gradio_format(history_before)
    history_after_internal = convert_from_gradio_format(history_after)

    # æ„å»ºæ¶ˆæ¯å†å²
    messages_before = []
    for user_msg, assistant_msg in history_before_internal:
        messages_before.append({"role": "user", "content": user_msg})
        messages_before.append({"role": "assistant", "content": assistant_msg})
    messages_before.append({"role": "user", "content": message})

    messages_after = []
    for user_msg, assistant_msg in history_after_internal:
        messages_after.append({"role": "user", "content": user_msg})
        messages_after.append({"role": "assistant", "content": assistant_msg})
    messages_after.append({"role": "user", "content": message})

    # åŒæ—¶ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„å“åº”
    full_response_before = ""
    full_response_after = ""

    # åˆ›å»ºç”Ÿæˆå™¨
    gen_before = client_before.stream_generation(
        messages=messages_before,
        temperature=temperature,
        max_tokens=max_tokens
    )

    gen_after = client_after.stream_generation(
        messages=messages_after,
        temperature=temperature,
        max_tokens=max_tokens
    )

    # äº¤æ›¿è·å–ä¸¤ä¸ªæ¨¡å‹çš„å“åº”
    while True:
        try:
            chunk_before = next(gen_before)
            full_response_before += chunk_before
        except StopIteration:
            pass

        try:
            chunk_after = next(gen_after)
            full_response_after += chunk_after
        except StopIteration:
            pass

        # è½¬æ¢ä¸º Gradio æ ¼å¼
        history_before_new = history_before_internal + [
            [message, full_response_before]]
        history_after_new = history_after_internal + [
            [message, full_response_after]]

        gradio_before = convert_to_gradio_format(history_before_new)
        gradio_after = convert_to_gradio_format(history_after_new)

        # æ£€æŸ¥æ˜¯å¦éƒ½å®Œæˆç”Ÿæˆ
        both_done = False
        try:
            next(gen_before)
        except StopIteration:
            try:
                next(gen_after)
            except StopIteration:
                both_done = True

        yield gradio_before, gradio_after

        if both_done:
            break


def stream_chat_single(message, gradio_history, temperature, max_tokens,
                       client_type="before"):
    """å•æ¨¡å‹æµå¼èŠå¤©å‡½æ•°"""
    client = client_before if client_type == "before" else client_after

    # å°† Gradio æ ¼å¼è½¬æ¢å›å†…éƒ¨æ ¼å¼
    history = convert_from_gradio_format(gradio_history)

    # æ„å»ºæ¶ˆæ¯å†å²
    messages = []
    for user_msg, assistant_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": assistant_msg})
    messages.append({"role": "user", "content": message})

    # æµå¼ç”Ÿæˆ
    full_response = ""
    for chunk in client.stream_generation(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
    ):
        full_response += chunk
        # è½¬æ¢ä¸º Gradio æ ¼å¼
        history_new = history + [[message, full_response]]
        gradio_history_new = convert_to_gradio_format(history_new)
        yield gradio_history_new


def generate_text_compare(prompt_text, temp, tokens):
    """å¯¹æ¯”æ–‡æœ¬ç”Ÿæˆ"""
    messages = [{"role": "user", "content": prompt_text}]

    result_before = ""
    result_after = ""

    # ç”Ÿæˆå¾®è°ƒå‰æ¨¡å‹å“åº”
    for chunk in client_before.stream_generation(
            messages=messages,
            temperature=temp,
            max_tokens=tokens
    ):
        result_before += chunk

    # ç”Ÿæˆå¾®è°ƒåæ¨¡å‹å“åº”
    for chunk in client_after.stream_generation(
            messages=messages,
            temperature=temp,
            max_tokens=tokens
    ):
        result_after += chunk

    return result_before, result_after


# åˆ›å»ºé«˜çº§ç•Œé¢
with gr.Blocks(theme=gr.themes.Monochrome(),
               title="æ¨¡å‹å¾®è°ƒå¯¹æ¯”ç³»ç»Ÿ") as advanced_demo:
    gr.Markdown("""
    # ğŸ¤– æ¨¡å‹å¾®è°ƒå¯¹æ¯”ç³»ç»Ÿ

    **å¯¹æ¯”å¾®è°ƒå‰åæ¨¡å‹çš„æ€§èƒ½å·®å¼‚**
    """)

    with gr.Tab("ğŸ”„ å®æ—¶å¯¹æ¯”æ¨¡å¼"):
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### å‚æ•°é…ç½®")
                temperature = gr.Slider(0.1, 2.0, value=0.7, label="åˆ›é€ æ€§")
                max_tokens = gr.Slider(100, 2000, value=800, label="ç”Ÿæˆé•¿åº¦")
                top_p = gr.Slider(0.1, 1.0, value=0.9, label="Top-P")

                # æ¨¡å‹ä¿¡æ¯å±•ç¤º
                gr.Markdown("### ğŸ“‹ æ¨¡å‹ä¿¡æ¯")
                with gr.Group():
                    gr.Markdown(f"**{MODEL_NAMES['before']['display_name']}**")
                    gr.Markdown(MODEL_NAMES['before']['server_info'])
                    gr.Markdown(MODEL_NAMES['before']['model_info'])

                    gr.Markdown("---")

                    gr.Markdown(f"**{MODEL_NAMES['after']['display_name']}**")
                    gr.Markdown(MODEL_NAMES['after']['server_info'])
                    gr.Markdown(MODEL_NAMES['after']['model_info'])

            with gr.Column(scale=2):
                gr.Markdown(f"### {MODEL_NAMES['before']['display_name']}")
                chatbot_before = gr.Chatbot(
                    label=f"{MODEL_NAMES['before']['display_name']} - {MODEL_NAMES['before']['model_info']}",
                    height=400,
                    type="messages",
                    show_copy_button=True
                )

            with gr.Column(scale=2):
                gr.Markdown(f"### {MODEL_NAMES['after']['display_name']}")
                chatbot_after = gr.Chatbot(
                    label=f"{MODEL_NAMES['after']['display_name']} - {MODEL_NAMES['after']['model_info']}",
                    height=400,
                    type="messages",
                    show_copy_button=True
                )

        with gr.Row():
            msg_compare = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="è¾“å…¥é—®é¢˜ï¼ŒåŒæ—¶å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”...",
                scale=4
            )
            send_compare_btn = gr.Button("å‘é€å¯¹æ¯”", scale=1,
                                         variant="primary")

    with gr.Tab("ğŸ“Š ç‹¬ç«‹æµ‹è¯•æ¨¡å¼"):
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### å‚æ•°é…ç½®")
                temp_single = gr.Slider(0.1, 2.0, value=0.7, label="åˆ›é€ æ€§")
                max_tokens_single = gr.Slider(100, 2000, value=800,
                                              label="ç”Ÿæˆé•¿åº¦")
                model_choice = gr.Radio(
                    choices=[
                        f"{MODEL_NAMES['before']['display_name']} ({MODEL_NAMES['before']['model_info']})",
                        f"{MODEL_NAMES['after']['display_name']} ({MODEL_NAMES['after']['model_info']})"
                    ],
                    label="é€‰æ‹©æµ‹è¯•æ¨¡å‹",
                    value=f"{MODEL_NAMES['before']['display_name']} ({MODEL_NAMES['before']['model_info']})"
                )
                clear_single_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")

                # å½“å‰é€‰ä¸­æ¨¡å‹ä¿¡æ¯
                gr.Markdown("### ğŸ” å½“å‰æ¨¡å‹ä¿¡æ¯")
                model_info_display = gr.Markdown()

            with gr.Column(scale=2):
                chatbot_single = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=500,
                    type="messages",
                    show_copy_button=True
                )

        with gr.Row():
            msg_single = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="ä¸é€‰å®šæ¨¡å‹å¯¹è¯...",
                scale=4
            )
            send_single_btn = gr.Button("å‘é€", scale=1, variant="primary")

    with gr.Tab("ğŸ“ æ–‡æœ¬ç”Ÿæˆå¯¹æ¯”"):
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ç”Ÿæˆå‚æ•°")
                temp_gen = gr.Slider(0.1, 2.0, value=0.7, label="åˆ›é€ æ€§")
                max_tokens_gen = gr.Slider(100, 2000, value=800,
                                           label="ç”Ÿæˆé•¿åº¦")
                generate_compare_btn = gr.Button("ç”Ÿæˆå¯¹æ¯”", variant="primary")

                # æ¨¡å‹ä¿¡æ¯
                gr.Markdown("### ğŸ“‹ å¯¹æ¯”æ¨¡å‹")
                with gr.Group():
                    gr.Markdown(f"**{MODEL_NAMES['before']['display_name']}**")
                    gr.Markdown(MODEL_NAMES['before']['server_info'])
                    gr.Markdown(MODEL_NAMES['before']['model_info'])

                    gr.Markdown("---")

                    gr.Markdown(f"**{MODEL_NAMES['after']['display_name']}**")
                    gr.Markdown(MODEL_NAMES['after']['server_info'])
                    gr.Markdown(MODEL_NAMES['after']['model_info'])

            with gr.Column(scale=1):
                prompt_compare = gr.Textbox(
                    label="è¾“å…¥æç¤ºè¯",
                    placeholder="æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å†…å®¹...",
                    lines=5
                )

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown(f"### {MODEL_NAMES['before']['display_name']}")
                generated_before = gr.Textbox(
                    label=f"{MODEL_NAMES['before']['model_info']}",
                    lines=8,
                    show_copy_button=True
                )

            with gr.Column(scale=1):
                gr.Markdown(f"### {MODEL_NAMES['after']['display_name']}")
                generated_after = gr.Textbox(
                    label=f"{MODEL_NAMES['after']['model_info']}",
                    lines=8,
                    show_copy_button=True
                )

        clear_gen_btn = gr.Button("æ¸…ç©ºæ‰€æœ‰", variant="secondary")


    # æ›´æ–°æ¨¡å‹ä¿¡æ¯æ˜¾ç¤ºçš„å‡½æ•°
    def update_model_info(choice):
        if "å¾®è°ƒå‰æ¨¡å‹" in choice:
            return f"""
            **å½“å‰é€‰ä¸­æ¨¡å‹ä¿¡æ¯:**
            - {MODEL_NAMES['before']['display_name']}
            - {MODEL_NAMES['before']['server_info']}
            - {MODEL_NAMES['before']['model_info']}
            """
        else:
            return f"""
            **å½“å‰é€‰ä¸­æ¨¡å‹ä¿¡æ¯:**
            - {MODEL_NAMES['after']['display_name']}
            - {MODEL_NAMES['after']['server_info']}
            - {MODEL_NAMES['after']['model_info']}
            """


    # äº‹ä»¶å¤„ç† - å®æ—¶å¯¹æ¯”æ¨¡å¼
    send_compare_btn.click(
        fn=stream_chat_compare,
        inputs=[msg_compare, chatbot_before, chatbot_after, temperature,
                max_tokens],
        outputs=[chatbot_before, chatbot_after]
    ).then(lambda: "", outputs=msg_compare)


    # äº‹ä»¶å¤„ç† - ç‹¬ç«‹æµ‹è¯•æ¨¡å¼
    def get_single_model_type(choice):
        return "before" if "å¾®è°ƒå‰æ¨¡å‹" in choice else "after"


    # ç»‘å®šæ¨¡å‹é€‰æ‹©å˜åŒ–äº‹ä»¶
    model_choice.change(
        fn=update_model_info,
        inputs=model_choice,
        outputs=model_info_display
    )

    # åˆå§‹åŒ–æ¨¡å‹ä¿¡æ¯æ˜¾ç¤º
    advanced_demo.load(
        fn=lambda: update_model_info(model_choice.value),
        outputs=model_info_display
    )

    send_single_btn.click(
        fn=stream_chat_single,
        inputs=[msg_single, chatbot_single, temp_single, max_tokens_single,
                gr.State(value=get_single_model_type(model_choice.value))],
        outputs=chatbot_single
    ).then(lambda: "", outputs=msg_single)

    # æ¸…ç©ºç‹¬ç«‹æµ‹è¯•æ¨¡å¼å¯¹è¯
    clear_single_btn.click(
        fn=lambda: [],
        outputs=[chatbot_single]
    )

    # äº‹ä»¶å¤„ç† - æ–‡æœ¬ç”Ÿæˆå¯¹æ¯”
    generate_compare_btn.click(
        fn=generate_text_compare,
        inputs=[prompt_compare, temp_gen, max_tokens_gen],
        outputs=[generated_before, generated_after]
    )


    # æ¸…ç©ºæ–‡æœ¬ç”Ÿæˆå¯¹æ¯”
    def clear_all():
        return "", "", "", ""


    clear_gen_btn.click(
        fn=clear_all,
        outputs=[prompt_compare, generated_before, generated_after]
    )

    # æ·»åŠ ä¸€äº›ä½¿ç”¨è¯´æ˜
    with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
        gr.Markdown(f"""
        ## ä½¿ç”¨æŒ‡å—

        ### ğŸ”„ å®æ—¶å¯¹æ¯”æ¨¡å¼
        - åŒæ—¶å‘å¾®è°ƒå‰åçš„ä¸¤ä¸ªæ¨¡å‹å‘é€ç›¸åŒçš„é—®é¢˜
        - å®æ—¶è§‚å¯Ÿä¸¤ä¸ªæ¨¡å‹çš„å“åº”å·®å¼‚
        - é€‚åˆå¿«é€Ÿæ¯”è¾ƒæ¨¡å‹æ€§èƒ½

        ### ğŸ“Š ç‹¬ç«‹æµ‹è¯•æ¨¡å¼  
        - å•ç‹¬æµ‹è¯•æŸä¸ªæ¨¡å‹çš„æ€§èƒ½
        - å¯ä»¥æ›´æ·±å…¥åœ°äº†è§£ç‰¹å®šæ¨¡å‹çš„è¡Œä¸º
        - é€‚åˆè¯¦ç»†çš„åŠŸèƒ½æµ‹è¯•

        ### ğŸ“ æ–‡æœ¬ç”Ÿæˆå¯¹æ¯”
        - å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°
        - é€‚åˆåˆ›æ„å†™ä½œã€å†…å®¹ç”Ÿæˆç­‰åœºæ™¯

        ## æ¨¡å‹ä¿¡æ¯
        - **{MODEL_NAMES['before']['display_name']}**: 
          - {MODEL_NAMES['before']['server_info']}
          - {MODEL_NAMES['before']['model_info']}

        - **{MODEL_NAMES['after']['display_name']}**: 
          - {MODEL_NAMES['after']['server_info']}
          - {MODEL_NAMES['after']['model_info']}
        """)

if __name__ == "__main__":
    advanced_demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True
    )